{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "width, height = 10, 5\n",
    "mpl.rcParams['figure.figsize'] = [width, height]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "\n",
    "from utility_fns import form_last_n_games\n",
    "from utility_fns import make_train_val_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = os.path.dirname(os.getcwd())\n",
    "data = os.path.join(basedir, 'data', 'derived', 'cleaned_matches.csv')\n",
    "defense = os.path.join(basedir, 'data', 'derived', 'stats_categories', 'DEFENSE_stats.csv')\n",
    "snaps = os.path.join(basedir, 'data', 'derived', 'stats_categories', 'SNAP_COUNTS_stats.csv')\n",
    "\n",
    "cleaned_matches = pd.read_csv(data)\n",
    "# defense_stats = pd.read_csv(defense, index_col=[0,1,2])\n",
    "# snaps_stats = pd.read_csv(snaps, index_col=[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_matches.Date = pd.to_datetime(cleaned_matches.Date)\n",
    "cleaned_matches.sort_values(by='Date', inplace=True, ascending=True)\n",
    "cleaned_matches.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_matches['Pct Team Score'] = cleaned_matches['Team Score'] / (cleaned_matches['Team Score'] + cleaned_matches['Opponent Score'])\n",
    "cleaned_matches['Pct Opponent Score'] = cleaned_matches['Opponent Score'] / (cleaned_matches['Team Score'] + cleaned_matches['Opponent Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train to classify based on the last-n-games a team played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = [\n",
    "    'Team Code',\n",
    "    'Opponent Code',\n",
    "    'Location'\n",
    "]\n",
    "lookup_x_cols = [\n",
    "    'Team Score',\n",
    "    'Opponent Score',\n",
    "    'Pct Team Score',\n",
    "    'Pct Opponent Score',\n",
    "    'Location'\n",
    "]\n",
    "key_x_cols = [\n",
    "  'Team Code',\n",
    "  'Opponent Code'\n",
    "]\n",
    "y_cols = [\n",
    "    'Class'\n",
    "]\n",
    "\n",
    "played_matches = cleaned_matches.dropna(\n",
    "    axis=0, how='any', subset=x_cols + y_cols + lookup_x_cols).copy()\n",
    "\n",
    "played_matches['Class'].where(\n",
    "    played_matches['Class'] > 0, other=0, inplace=True)\n",
    "\n",
    "\n",
    "played_matches.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_n_matches, new_columns = form_last_n_games(\n",
    "    played_matches, 5, lookup_x_cols, key_x_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([last_n_matches[x_cols], last_n_matches[new_columns]], axis=1)\n",
    "y = last_n_matches[y_cols]\n",
    "\n",
    "numeric_columns = X.columns[X.columns.str.contains('Score')]\n",
    "dummies_columns = X.columns[~X.columns.str.contains('Score')]\n",
    "print(dummies_columns)\n",
    "print(numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_transformer():\n",
    "    return ColumnTransformer([('scaler', StandardScaler(), numeric_columns), ('one-hot', OneHotEncoder(handle_unknown='ignore'), dummies_columns)])\n",
    "\n",
    "dataset_transformer().fit(X, y).transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = make_train_val_test(\n",
    "    X, y, test_pct=0.2, val_pct=0.2, random_state=0\n",
    ")\n",
    "\n",
    "# y_train = y_train.ravel()\n",
    "# y_val = y_val.ravel()\n",
    "# y_test = y_test.ravel()\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "    train_scores = []\n",
    "    val_scores = []\n",
    "    val_space = np.geomspace(start=0.01, stop=10, num=3)\n",
    "    for C in val_space:\n",
    "        print(f\"Evaluating {kernel} kernel for C={C}\")\n",
    "        model = make_pipeline(dataset_transformer(), svm.SVC(C=C, max_iter=10_000, kernel=kernel))\n",
    "\n",
    "        model.fit(X_train, y_train.values.ravel())\n",
    "        train_scores.append(model.score(X_train, y_train.values.ravel()))\n",
    "        val_scores.append(model.score(X_val, y_val.values.ravel()))\n",
    "\n",
    "    plt.title(f'Best C for {kernel}: {val_space[np.argmax(val_scores)]}')\n",
    "    plt.plot(val_space, train_scores, label='Train score')\n",
    "    plt.plot(val_space, val_scores, label='Val score')\n",
    "    # plt.ylim(0, 1.1)\n",
    "    plt.xscale('log')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "precision, recall, fbeta_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config = []\n",
    "best_score = 0\n",
    "best_fit_config = []\n",
    "best_fit_score = 0\n",
    "for estimator in [DecisionTreeClassifier(max_leaf_nodes=3), DecisionTreeClassifier(max_leaf_nodes=5), DecisionTreeClassifier(max_leaf_nodes=10), DecisionTreeClassifier(max_leaf_nodes=50)]:\n",
    "  for nestimators in [10, 31, 62, 93, 124, 200, 300]:\n",
    "    for nsamples in [10, 20, 30, 50, 75, 100]:\n",
    "      clf = make_pipeline(dataset_transformer(), BaggingClassifier(base_estimator=estimator, n_estimators=nestimators, max_samples=nsamples))\n",
    "      clf.fit(X_train, y_train.values.ravel())\n",
    "      \n",
    "      score = clf.score(X_val, y_val.values.ravel())\n",
    "      if(score > best_score):\n",
    "        best_config = [estimator, nestimators, nsamples]\n",
    "        best_score = score\n",
    "      \n",
    "      score = clf.score(X_train, y_train)\n",
    "      if(score > best_fit_score):\n",
    "        best_fit_config = [estimator, nestimators, nsamples]\n",
    "        best_fit_score = score\n",
    "        \n",
    "      print(end='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best validation: {best_score}, {best_config}\")\n",
    "print(f\"Best training: {best_fit_score}, {best_fit_config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(dataset_transformer(), DecisionTreeClassifier())\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(clf.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cv = pd.concat([X_train, X_val])\n",
    "y_cv = pd.concat([y_train, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = np.geomspace(0.01, 15, num=75)\n",
    "scores = []\n",
    "for c in Cs:\n",
    "    model = make_pipeline(dataset_transformer(), svm.LinearSVC(dual=False, C=c, max_iter=100_000))\n",
    "    scores.append(cross_val_score(\n",
    "        model, X_cv, y_cv.values.ravel(), cv=5))\n",
    "    print(end='.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Cs\n",
    "plt.errorbar(x, np.mean(scores,axis=1), np.std(scores,axis=1), label='Mean & Std Dev. of CV Score', linewidth=1)\n",
    "plt.plot(x, np.sort(scores, axis=1), linewidth=0.2)\n",
    "plt.legend()\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Cross-Validation Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Maximum mean:', np.max(np.mean(scores,axis=1)))\n",
    "print('Corresponding std deviation:', np.std(scores,axis=1)[np.argmax(np.mean(scores,axis=1))])\n",
    "print('C:',Cs[np.argmax(np.mean(scores,axis=1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Minimum std deviation:', np.min(np.std(scores,axis=1)))\n",
    "print('Corresponding mean:', np.mean(scores,axis=1)[np.argmin(np.std(scores,axis=1))])\n",
    "print('C:',Cs[np.argmin(np.std(scores,axis=1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mean_cs = np.flip(np.argsort(np.mean(scores,axis=1)))\n",
    "best_std_cs = np.argsort(np.std(scores,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some random perturbation of the Cs\n",
    "scores = []\n",
    "\n",
    "Cs_to_examine = np.unique(np.concatenate(\n",
    "    (Cs[best_mean_cs[:5]], Cs[best_std_cs[:5]])))\n",
    "Cs_to_examine = np.unique(np.concatenate([Cs_to_examine, np.abs(Cs_to_examine + np.random.normal(scale=0.2,size=len(Cs_to_examine)))]))\n",
    "\n",
    "for c in Cs_to_examine:\n",
    "    model = make_pipeline(dataset_transformer(), svm.LinearSVC(dual=False, C=c, max_iter=100_000))\n",
    "    \n",
    "    scores.append(cross_val_score(\n",
    "        model, X_cv, y_cv.values.ravel(), cv=100, n_jobs=-1))\n",
    "    \n",
    "    print(end='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Cs_to_examine\n",
    "quantiles = [0, 0.1, 0.2, 0.3, 0.4, 0.6, 0.8, 1.0]\n",
    "plt.errorbar(x, np.mean(scores,axis=1), np.std(scores,axis=1), label='Mean & Std Dev. of CV Score', linewidth=1)\n",
    "plt.plot(x, np.quantile(scores, quantiles, axis=1).T, linewidth=0.2)\n",
    "plt.legend()\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Cross-Validation Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mean_idx = np.argmax(np.mean(scores,axis=1))\n",
    "\n",
    "print(f'Best C={Cs_to_examine[best_mean_idx]} with mean={np.mean(scores,axis=1)[best_mean_idx]}, std={np.std(scores,axis=1)[best_mean_idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the best model.\n",
    "\n",
    "model = make_pipeline(dataset_transformer(),\n",
    "    svm.LinearSVC(dual=False, C=Cs_to_examine[best_mean_idx], max_iter=100_000))\n",
    "model.fit(X_cv, y_cv.values.ravel())\n",
    "model.score(X_test, y_test.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = model[1].coef_[0]\n",
    "plt.scatter(x=np.arange(len(coefs)),y=coefs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y.values.ravel())\n",
    "confidences = model.decision_function(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(confidences, bins=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(confidences * (2 * y.to_numpy().ravel() - 1), bins=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(confidences * (2 * y.to_numpy().ravel() - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "43ec02794a5c6caca2187426f9bfbca63869e8dd5c007b90979db8c1bb48c23c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('machine_learning': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
